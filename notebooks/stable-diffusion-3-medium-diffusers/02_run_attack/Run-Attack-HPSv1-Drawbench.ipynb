{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pep10KJ56zU4"
      },
      "source": [
        "1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7WLA6_b66tWX",
        "outputId": "c3ee11b8-e520-4f8d-ca67-fb2541ac7503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jypb_0_u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-jypb_0_u\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=e6267ae6a8aaa6032e700e256cc4aceea9a4fbb7f41e9db7a607adc738fdf6ce\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iru66hpk/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting hpsv2\n",
            "  Downloading hpsv2-1.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from hpsv2) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from hpsv2) (0.20.1+cu124)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from hpsv2) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from hpsv2) (6.3.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from hpsv2) (0.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from hpsv2) (2.2.2)\n",
            "Collecting braceexpand (from hpsv2)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from hpsv2) (2024.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from hpsv2) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from hpsv2) (0.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from hpsv2) (0.2.0)\n",
            "Collecting protobuf<4 (from hpsv2)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from hpsv2) (1.0.15)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from hpsv2) (4.48.3)\n",
            "Collecting webdataset (from hpsv2)\n",
            "  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from hpsv2) (18.1.0)\n",
            "Collecting pytest-split==0.8.0 (from hpsv2)\n",
            "  Downloading pytest_split-0.8.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting pytest==7.2.0 (from hpsv2)\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from hpsv2) (2.32.3)\n",
            "Collecting clint (from hpsv2)\n",
            "  Downloading clint-0.5.1.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from pytest==7.2.0->hpsv2) (25.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==7.2.0->hpsv2) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest==7.2.0->hpsv2) (24.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from pytest==7.2.0->hpsv2) (1.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->hpsv2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->hpsv2) (1.3.0)\n",
            "Collecting args (from clint->hpsv2)\n",
            "  Downloading args-0.1.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->hpsv2) (0.2.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->hpsv2) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->hpsv2) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->hpsv2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->hpsv2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->hpsv2) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->hpsv2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->hpsv2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->hpsv2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->hpsv2) (2025.1.31)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->hpsv2) (0.5.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->hpsv2) (11.1.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->hpsv2) (0.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->hpsv2) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->hpsv2) (3.0.2)\n",
            "Downloading hpsv2-1.2.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_split-0.8.0-py3-none-any.whl (11 kB)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clint, args\n",
            "  Building wheel for clint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clint: filename=clint-0.5.1-py3-none-any.whl size=34459 sha256=40ce39747b011fd72cfa6783111b3b3ebbcd4b402494c8788220ecd1270a1a5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/8d/f3/91dd49f9a8c6a57be7715f6d11347c49971dd292a53397ed79\n",
            "  Building wheel for args (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for args: filename=args-0.1.0-py3-none-any.whl size=3319 sha256=174d96db1bb516d20af4d06d102078a7afd947556c353a48ecb7185341687759\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/a2/87/2541eb895fd18fd20cc7dd18b14d3b61bd9084cf4322abd15e\n",
            "Successfully built clint args\n",
            "Installing collected packages: braceexpand, args, webdataset, pytest, protobuf, clint, pytest-split, hpsv2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.3.5\n",
            "    Uninstalling pytest-8.3.5:\n",
            "      Successfully uninstalled pytest-8.3.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed args-0.1.0 braceexpand-0.1.7 clint-0.5.1 hpsv2-1.2.0 protobuf-3.20.3 pytest-7.2.0 pytest-split-0.8.0 webdataset-0.2.111\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ed378ee7e18c44bdac9dc92a38eaed91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.1.31)\n",
            "Collecting torchattacks\n",
            "  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (0.20.1+cu124)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (4.67.1)\n",
            "Collecting requests~=2.25.1 (from torchattacks)\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (1.26.4)\n",
            "Collecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.25.1->torchattacks) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->torchattacks) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.8.2->torchattacks) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (3.0.2)\n",
            "Downloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, idna, chardet, requests, torchattacks\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.25.1 which is incompatible.\n",
            "yfinance 0.2.54 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\n",
            "google-genai 1.2.0 requires requests<3.0.0dev,>=2.28.1, but you have requests 2.25.1 which is incompatible.\n",
            "bigframes 1.38.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\n",
            "sphinx 8.1.3 requires requests>=2.30.0, but you have requests 2.25.1 which is incompatible.\n",
            "tweepy 4.15.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 requests-2.25.1 torchattacks-3.5.1 urllib3-1.26.20\n"
          ]
        }
      ],
      "source": [
        "# HPS dependencies\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install hpsv2\n",
        "\n",
        "# Stable Diffusion dependencies\n",
        "! pip install diffusers\n",
        "\n",
        "# Adversarial attack dependencies\n",
        "! pip install torchattacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJrLntGwVr1U",
        "outputId": "b8562bf8-086f-4221-f65d-757f0267c7d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-09 05:16:59--  https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/openai/CLIP/main/clip/bpe_simple_vocab_16e6.txt.gz [following]\n",
            "--2025-03-09 05:16:59--  https://raw.githubusercontent.com/openai/CLIP/main/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘/usr/local/lib/python3.11/dist-packages/hpsv2/src/open_clip/bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-09 05:16:59 (53.8 MB/s) - ‘/usr/local/lib/python3.11/dist-packages/hpsv2/src/open_clip/bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p clip && wget https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz -P /usr/local/lib/python3.11/dist-packages/hpsv2/src/open_clip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B1nqX-8HEsUH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tTOq5ns8eSs"
      },
      "source": [
        "2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "46c5e568aa7643f8b16d6b1b0d0efd3b",
            "f9c099061f8f445380058f199c5ef60d",
            "c05b5360df3a4c76a31a82c34172aa3f",
            "49599243926646419c2884e4924b3ccf",
            "b19e7adf6e7b459898f81e452be8af19",
            "a1a94b3b693047d9aad25af39bffcead",
            "48b87674d3f247c398924a6e08310da3",
            "d154a3e810c7489fa118b4dde220e765",
            "f446e65889c04727873ca716257afc61",
            "350e831c3e9344fcaed2583c3b73f029",
            "09bccfe2c1424bc4b59276418e99f099"
          ]
        },
        "id": "mBkPPyEw8fNB",
        "outputId": "35c972e3-5f68-4bf4-937b-8e9a73c2771d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46c5e568aa7643f8b16d6b1b0d0efd3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import ast\n",
        "from datetime import datetime\n",
        "import random\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from collections import OrderedDict\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from diffusers import DiffusionPipeline, StableDiffusionPipeline, StableDiffusion3Pipeline\n",
        "\n",
        "import clip\n",
        "import hpsv2\n",
        "from hpsv2.src.open_clip import create_model_and_transforms, get_tokenizer\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "from torchattacks.attack import Attack, wrapper_method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEjuf1ym7e3r"
      },
      "source": [
        "3. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6a3bh3De8hV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d2a628-8926-4ebb-9000-cf1a5d935a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\",force_remount=True)\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YReKB8Hv7Uw6"
      },
      "source": [
        "4. Model Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j02uLImH0ubF"
      },
      "outputs": [],
      "source": [
        "class ModelLoadingError(Exception):\n",
        "    \"\"\"Exception raised when there is an error loading the model.\"\"\"\n",
        "    pass\n",
        "\n",
        "class InferenceError(Exception):\n",
        "    \"\"\"Exception raised when an error occurs during inference.\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l1B1q8937Xw1"
      },
      "outputs": [],
      "source": [
        "class BaseModel(ABC):\n",
        "    @abstractmethod\n",
        "    def load_model(self):\n",
        "        \"\"\"\n",
        "        Load the open-weights model or make an API connection to the closed-source model.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def inference(\n",
        "        self, inputs: Union[List[str], torch.Tensor], captions: Optional[List[str]] = None\n",
        "    ) -> Union[torch.Tensor, List[float]]:\n",
        "        \"\"\"\n",
        "        Run inference on a batch of inputs with optional captions.\n",
        "\n",
        "        Args:\n",
        "            inputs (Union[List[str], torch.Tensor]): A batch of text prompts or a batch of images.\n",
        "            captions (Optional[List[str]]): Optional text captions associated with the inputs for reward models.\n",
        "\n",
        "        Returns:\n",
        "            Union[torch.Tensor, List[float]]: A batch of model outputs or a list of reward scores.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rUVTXwxc7Ybm"
      },
      "outputs": [],
      "source": [
        "class HPSv1Model(BaseModel):\n",
        "    def __init__(self, model_path: str):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_path (str): Path to the HPSv1 model checkpoint.\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model_path = model_path\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            self.model, self.preprocess_function = clip.load(\"ViT-L/14\", device=self.device)\n",
        "            checkpoint = torch.load(self.model_path)\n",
        "\n",
        "            if \"state_dict\" not in checkpoint:\n",
        "                raise ModelLoadingError(\"Checkpoint does not contain 'state_dict'.\")\n",
        "\n",
        "            self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "            self.tokenizer = clip.tokenize\n",
        "            self.model.eval()\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            raise ModelLoadingError(f\"Model checkpoint not found at '{self.model_path}'.\") from e\n",
        "        except Exception as e:\n",
        "            raise ModelLoadingError(f\"Error loading model: {e}\") from e\n",
        "\n",
        "    def inference(self, inputs: torch.Tensor, captions: Union[List[str], torch.Tensor]) -> List[float]:\n",
        "        \"\"\"\n",
        "        Runs inference on a batch of images and corresponding captions.\n",
        "        Returns a batch of reward scores.\n",
        "        \"\"\"\n",
        "        if not isinstance(inputs, torch.Tensor):\n",
        "            raise TypeError(\"Expected 'inputs' to be of type torch.Tensor (i.e. images).\")\n",
        "        if not (isinstance(captions, torch.Tensor) or (isinstance(captions, list) and all(isinstance(c, str) for c in captions))):\n",
        "            raise TypeError(\"Expected 'captions' to be either a torch.Tensor or a list of strings.\")\n",
        "        if inputs.shape[0] != len(captions):\n",
        "            raise ValueError(\"Number of 'inputs' and 'captions' must match.\")\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                image_features = self.model.encode_image(inputs.to(self.device))\n",
        "\n",
        "                if not isinstance(captions, torch.Tensor):\n",
        "                    text_tokens = self.tokenizer(captions).to(self.device)\n",
        "                else:\n",
        "                    text_tokens = captions.to(self.device)\n",
        "                text_features = self.model.encode_text(text_tokens)\n",
        "\n",
        "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Convert cosine similarity scores to percentages as in the original paper\n",
        "                similarity_scores = (image_features @ text_features.T).diag() * 100\n",
        "            return similarity_scores.tolist()\n",
        "        except Exception as e:\n",
        "            raise InferenceError(f\"Inference failed: {e}\") from e\n",
        "\n",
        "    def inference_with_grad(self, inputs: torch.Tensor, captions: List[str]) -> List[float]:\n",
        "        \"\"\"\n",
        "        Runs inference on a batch of images and corresponding captions.\n",
        "        Returns a batch of reward scores.\n",
        "        \"\"\"\n",
        "        if not isinstance(inputs, torch.Tensor):\n",
        "            raise TypeError(\"Expected 'inputs' to be of type torch.Tensor (i.e. images).\")\n",
        "        if not isinstance(captions, list) or not all(isinstance(c, str) for c in captions):\n",
        "            raise TypeError(\"Expected 'captions' to be a list of strings.\")\n",
        "        if inputs.shape[0] != len(captions):\n",
        "            raise ValueError(\"Number of 'inputs' and 'captions' must match.\")\n",
        "\n",
        "        try:\n",
        "            text_tokens = clip.tokenize(captions).to(self.device)\n",
        "            image_features, text_features = self.model(inputs, text_tokens)\n",
        "            return (image_features @ text_features.T).diag() * 100\n",
        "        except Exception as e:\n",
        "            raise InferenceError(f\"Inference failed: {e}\") from e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "33IxgoCcTkQ5"
      },
      "outputs": [],
      "source": [
        "class HPSv2Model(BaseModel):\n",
        "    def __init__(self, model_path: str):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_path (str): Path to the HPSv2 model checkpoint.\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model_path = model_path\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            self.model, _, self.preprocess_function = create_model_and_transforms(\n",
        "                \"ViT-H-14\",\n",
        "                \"laion2B-s32B-b79K\",\n",
        "                precision=\"amp\",\n",
        "                device=self.device,\n",
        "                jit=False,\n",
        "                force_quick_gelu=False,\n",
        "                force_custom_text=False,\n",
        "                force_patch_dropout=False,\n",
        "                force_image_size=None,\n",
        "                pretrained_image=False,\n",
        "                image_mean=None,\n",
        "                image_std=None,\n",
        "                light_augmentation=True,\n",
        "                aug_cfg={},\n",
        "                output_dict=True,\n",
        "                with_score_predictor=False,\n",
        "                with_region_predictor=False\n",
        "            )\n",
        "\n",
        "            checkpoint = torch.load(self.model_path)\n",
        "            if \"state_dict\" not in checkpoint:\n",
        "                raise ModelLoadingError(\"Checkpoint does not contain 'state_dict'.\")\n",
        "\n",
        "            self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "            self.tokenizer = get_tokenizer(\"ViT-H-14\")\n",
        "            self.model.eval()\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            raise ModelLoadingError(f\"Model checkpoint not found at '{self.model_path}'.\") from e\n",
        "        except Exception as e:\n",
        "            raise ModelLoadingError(f\"Error loading model: {e}\") from e\n",
        "\n",
        "    def inference(self, inputs: torch.Tensor, captions: Union[List[str], torch.Tensor]) -> List[float]:\n",
        "        \"\"\"\n",
        "        Runs inference on a batch of images and corresponding captions.\n",
        "        Returns a batch of reward scores.\n",
        "        \"\"\"\n",
        "        if not isinstance(inputs, torch.Tensor):\n",
        "            raise TypeError(\"Expected 'inputs' to be a list of PIL.Image objects.\")\n",
        "        if not (isinstance(captions, torch.Tensor) or (isinstance(captions, list) and all(isinstance(c, str) for c in captions))):\n",
        "            raise TypeError(\"Expected 'captions' to be either a torch.Tensor or a list of strings.\")\n",
        "        if len(inputs) != len(captions):\n",
        "            raise ValueError(\"Number of 'inputs' and 'captions' must match.\")\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                if not isinstance(captions, torch.Tensor):\n",
        "                    text_tokens = self.tokenizer(captions).to(self.device)\n",
        "                else:\n",
        "                    text_tokens = captions.to(self.device)\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = self.model(inputs, text_tokens)\n",
        "                    image_features, text_features = outputs[\"image_features\"], outputs[\"text_features\"]\n",
        "                    similarity_scores = (image_features @ text_features.T).diag() * 100\n",
        "                return similarity_scores.tolist()\n",
        "\n",
        "        except Exception as e:\n",
        "            raise InferenceError(f\"Inference failed: {e}\") from e\n",
        "\n",
        "\n",
        "    def inference_with_grad(self, inputs: torch.Tensor, captions: List[str]) -> List[float]:\n",
        "        \"\"\"\n",
        "        Runs inference on a batch of images and corresponding captions.\n",
        "        Returns a batch of reward scores.\n",
        "        \"\"\"\n",
        "        if not isinstance(inputs, torch.Tensor):\n",
        "            raise TypeError(\"Expected 'inputs' to be a list of PIL.Image objects.\")\n",
        "        if not isinstance(captions, list) or not all(isinstance(c, str) for c in captions):\n",
        "            raise TypeError(\"Expected 'captions' to be a list of strings.\")\n",
        "        if len(inputs) != len(captions):\n",
        "            raise ValueError(\"Number of 'inputs' and 'captions' must match.\")\n",
        "\n",
        "        try:\n",
        "            text_tokens = self.tokenizer(captions).to(self.device)\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = self.model(inputs, text_tokens)\n",
        "                image_features, text_features = outputs[\"image_features\"], outputs[\"text_features\"]\n",
        "                return (image_features @ text_features.T).diag() * 100\n",
        "\n",
        "        except Exception as e:\n",
        "            raise InferenceError(f\"Inference failed: {e}\") from e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kVVBTkx4JaQf"
      },
      "outputs": [],
      "source": [
        "class BaseDiffusionModel(BaseModel):\n",
        "    def __init__(self, model_path: str, offload_to_cpu: bool = False, resolution: int = None, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_path (str): Path or repository ID of the diffusion model checkpoint.\n",
        "        \"\"\"\n",
        "        self.seed = 42\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model_path = model_path\n",
        "        self.offload_to_cpu = offload_to_cpu\n",
        "        self.resolution = resolution\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "        self.diffusion_pipeline = self._get_diffusion_pipeline()\n",
        "        self.load_model()\n",
        "\n",
        "    def _get_diffusion_pipeline(self):\n",
        "        \"\"\" Subclasses should override this to return the correct pipeline. \"\"\"\n",
        "        return DiffusionPipeline\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            self.model = self.diffusion_pipeline.from_pretrained(\n",
        "                self.model_path,\n",
        "                **self.kwargs\n",
        "            ).to(self.device)\n",
        "            if self.offload_to_cpu:\n",
        "                self.model.enable_model_cpu_offload()\n",
        "\n",
        "        except MemoryError as e:\n",
        "            if hasattr(self, \"model\"):\n",
        "                del self.model\n",
        "                torch.cuda.empty_cache()\n",
        "            raise ModelLoadingError(f\"Memory error occurred while loading the model. Consider using a smaller model: {e}\")\n",
        "        except FileNotFoundError as e:\n",
        "            raise ModelLoadingError(f\"Model checkpoint not found at '{self.model_path}'.\") from e\n",
        "        except Exception as e:\n",
        "            raise ModelLoadingError(f\"Failed to load diffusion model: {e}\") from e\n",
        "\n",
        "    def inference(\n",
        "        self, inputs: List[str], captions: Optional[List[str]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Runs inference on a batch of prompts.\n",
        "        Returns a batch of images corresponding to the prompts.\n",
        "        \"\"\"\n",
        "        if not isinstance(inputs, list) or not all(isinstance(c, str) for c in inputs):\n",
        "            raise TypeError(\"Expected 'inputs' to be a list of strings.\")\n",
        "\n",
        "        try:\n",
        "            # Create one generator per prompt to ensure reproducibility\n",
        "            generators = [\n",
        "                torch.Generator(self.device).manual_seed(self.seed) for _ in range(len(inputs))\n",
        "            ]\n",
        "            if self.resolution:\n",
        "                images = self.model(\n",
        "                    prompt=inputs, generator=generators,\n",
        "                    height=self.resolution, width=self.resolution # use 1:1 aspect ratio\n",
        "                ).images\n",
        "                return images\n",
        "            else:\n",
        "                images = self.model(\n",
        "                    prompt=inputs, generator=generators,\n",
        "                ).images\n",
        "                return images\n",
        "\n",
        "        except Exception as e:\n",
        "            raise InferenceError(f\"Inference failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QII1xUW_Dv7d"
      },
      "outputs": [],
      "source": [
        "class StableDiffusionModel(BaseDiffusionModel):\n",
        "    def __init__(self, model_path: str, offload_to_cpu: bool = False, resolution: int = None, **kwargs):\n",
        "        \"\"\"\n",
        "        Note:\n",
        "            model_path (str): Path to the Stable Diffusion model.\n",
        "                              Must include 'stable-diffusion-1', 'stable-diffusion-2', or 'stable-diffusion-3' after '<repo-owner>/'\n",
        "                              for simplicity.\n",
        "        \"\"\"\n",
        "\n",
        "        # Load the model with float16 precision.\n",
        "        # If your GPU supports torch.bfloat16 for lower memory usage with similar precision to FP32,\n",
        "        # consider switching the torch_dtype accordingly.\n",
        "        if \"torch_dtype\" not in kwargs:\n",
        "            kwargs[\"torch_dtype\"] = torch.float16\n",
        "        super().__init__(model_path, offload_to_cpu, resolution, **kwargs)\n",
        "\n",
        "    def _get_diffusion_pipeline(self):\n",
        "        version_tag = self.model_path.split(\"/\")[-1].lower()\n",
        "\n",
        "        if re.search(r'(stable-diffusion-?(v-?|v)?1(?:-\\d+)?)(.*)?$', version_tag):\n",
        "            return StableDiffusionPipeline\n",
        "        elif re.search(r'(stable-diffusion-?(v-?|v)?2(?:-\\d+)?)(.*)?$', version_tag):\n",
        "            return DiffusionPipeline\n",
        "        elif re.search(r'(stable-diffusion-?(v-?|v)?3(?:-\\d+)?)(.*)?$', version_tag):\n",
        "            return StableDiffusion3Pipeline\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Model path must match 'stable-diffusion-1', 'stable-diffusion-v1', 'stable-diffusion-v-1', \"\n",
        "                \"'stable-diffusion-2', 'stable-diffusion-v2', etc.\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ehIBzLG_ieeG"
      },
      "outputs": [],
      "source": [
        "class ModelFactory:\n",
        "    @staticmethod\n",
        "    def create_model(\n",
        "        model_type: str, model_path: str,\n",
        "        **kwargs,\n",
        "    ) -> BaseModel:\n",
        "        \"\"\"\n",
        "        Creates and returns an instance of a model subclass based on the model_type.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): The type of model to create. Supported values are:\n",
        "                - \"hpsv1\": For HPSv1 reward models.\n",
        "                - \"hpsv2\": For HPSv2 reward models.\n",
        "                - \"sd\": For stable diffusion text-to-image models.\n",
        "            model_path (str): The path or repository ID of the model checkpoint.\n",
        "\n",
        "        Returns:\n",
        "            BaseModel: An instance of the requested model.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an unsupported model_type is provided.\n",
        "        \"\"\"\n",
        "        if model_type == \"hpsv1\":\n",
        "            return HPSv1Model(model_path)\n",
        "        elif model_type == \"hpsv2\":\n",
        "            return HPSv2Model(model_path)\n",
        "        elif model_type == \"sd\":\n",
        "            return StableDiffusionModel(model_path, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported model type. Use 'sd' for stable diffusion models or 'hps' for HPS models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wq_hTQKfM9f"
      },
      "source": [
        "5. Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "br0iOYEMgiDj"
      },
      "outputs": [],
      "source": [
        "class DatasetFormatError(Exception):\n",
        "    \"\"\"Raised when the dataset format is incorrect.\"\"\"\n",
        "    pass\n",
        "\n",
        "class DatasetLoadingError(Exception):\n",
        "    \"\"\"Raised when the dataset fails to load properly.\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-aJsW0f8fOR8"
      },
      "outputs": [],
      "source": [
        "class BasePromptDataset(Dataset, ABC):\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.data = self.load_dataset()\n",
        "        except Exception as e:\n",
        "            raise DatasetLoadingError(f\"Failed to load dataset: {e}\")\n",
        "\n",
        "        if not isinstance(self.data, dict):\n",
        "            raise DatasetFormatError(f\"Expected 'load_dataset()' to return a dictionary, got '{type(self.data)}'.\")\n",
        "\n",
        "        for key, prompts in self.data.items():\n",
        "            if not isinstance(prompts, list) or not all(isinstance(p, str) for p in prompts):\n",
        "                raise DatasetFormatError(f\"Expected a list of strings for category '{key}', but got '{type(prompts)}'\")\n",
        "\n",
        "        # Precompute samples with round-robin ordering\n",
        "        self.samples = self._create_round_robin_samples()\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_dataset(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"To be implemented by subclasses.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def _create_round_robin_samples(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"Ensure fair round-robin interleaving of prompts from all categories.\"\"\"\n",
        "        samples = []\n",
        "        categories = list(self.data.keys())\n",
        "        category_prompts = [self.data[cat] for cat in categories]\n",
        "\n",
        "        if not categories or all(len(prompts) == 0 for prompts in category_prompts):\n",
        "            raise DatasetFormatError(\"Dataset is empty or contains only empty categories.\")\n",
        "\n",
        "        max_length = max(len(prompts) for prompts in category_prompts)\n",
        "\n",
        "        # Round-robin interleaving\n",
        "        for i in range(max_length):\n",
        "            for cat_idx, category in enumerate(categories):\n",
        "                prompts = category_prompts[cat_idx]\n",
        "                if len(prompts) > 0:\n",
        "                    prompt = prompts[i % len(prompts)]  # Cycle back for shorter lists\n",
        "                    samples.append({\"category\": category, \"prompt\": prompt})\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "    def num_categories(self) -> int:\n",
        "        \"\"\"Returns the number of unique categories in the dataset.\"\"\"\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iNuLke0ogt1o"
      },
      "outputs": [],
      "source": [
        "class HPSV2PromptDataset(BasePromptDataset):\n",
        "    def load_dataset(self) -> Dict[str, List[str]]:\n",
        "        all_prompts = hpsv2.benchmark_prompts(\"all\")\n",
        "        return dict(all_prompts.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rEbS3Tnngw8o"
      },
      "outputs": [],
      "source": [
        "class DrawBenchPromptDataset(BasePromptDataset):\n",
        "    def load_dataset(self) -> Dict[str, List[str]]:\n",
        "        df = pd.read_csv(\"drawbench_data.csv\")\n",
        "        return df.groupby(\"Category\")[\"Prompts\"].apply(list).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZkG1-SFKg8-t"
      },
      "outputs": [],
      "source": [
        "class ImagePromptDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            image_list: List[PIL.Image], prompt_list: List[Tuple[str, str]],\n",
        "            image_transform_function: callable, text_tokenizer_function: callable = None\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_list (List[PIL.Image]): List of PIL images.\n",
        "            prompt_list (List[Tuple[str, str]]): List of (category, prompt) tuples.\n",
        "            image_transform_function (callable): Function to transform PIL images.\n",
        "            text_tokenizer_function (callable): Function to tokenize text prompts.\n",
        "        \"\"\"\n",
        "        if len(image_list) == 0 or len(prompt_list) == 0:\n",
        "            raise DatasetFormatError(\"Both image_list and prompt_list must be non-empty.\")\n",
        "        if len(image_list) != len(prompt_list):\n",
        "            raise DatasetFormatError(\"Images and prompts must have the same length.\")\n",
        "\n",
        "        self.images = image_list\n",
        "        self.prompts = prompt_list  # List of (category, prompt)\n",
        "        self.image_transform_function = image_transform_function\n",
        "        self.text_tokenizer_function = text_tokenizer_function\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_transform_function(self.images[idx])\n",
        "        _, prompt = self.prompts[idx]\n",
        "        if self.text_tokenizer_function is None:\n",
        "            tokens = prompt\n",
        "        else:\n",
        "            tokens = self.text_tokenizer_function(prompt)\n",
        "\n",
        "        return image, tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4Dm6NzvhiO3E"
      },
      "outputs": [],
      "source": [
        "class RoundRobinSampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, dataset: BasePromptDataset):\n",
        "        self.dataset = dataset\n",
        "        self.indices = self._generate_indices()\n",
        "\n",
        "    def _generate_indices(self):\n",
        "        \"\"\"\n",
        "        Assume dataset.data has equal length lists per category.\n",
        "\n",
        "        For each category, create a shuffled list of indices corresponding to that category's samples.\n",
        "        Since BasePromptDataset precomputes samples in round-robin order, we need to map from category + position\n",
        "        to the flat sample index.\n",
        "\n",
        "        In our round-robin samples, the ordering is:\n",
        "        index 0: category1, index 1: category2, ..., index N: category1\n",
        "\n",
        "        Let K = number of categories,\n",
        "        Then the sample index for category j at position i is: i*K + j.\n",
        "        \"\"\"\n",
        "        categories = list(self.dataset.data.keys())\n",
        "        num_per_category = len(next(iter(self.dataset.data.values())))\n",
        "        K = len(categories)\n",
        "\n",
        "        category_indices = {}\n",
        "        for j, cat in enumerate(categories):\n",
        "            indices = [i * K + j for i in range(num_per_category)]\n",
        "            random.shuffle(indices)\n",
        "            category_indices[cat] = indices\n",
        "\n",
        "        ordered_indices = []\n",
        "        for i in range(num_per_category):\n",
        "            for cat in categories:\n",
        "                ordered_indices.append(category_indices[cat][i])\n",
        "        return ordered_indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xMV2IXqaibKB"
      },
      "outputs": [],
      "source": [
        "class DatasetFactory:\n",
        "    @staticmethod\n",
        "    def create_dataset(\n",
        "        dataset_type: str,\n",
        "        **kwargs,\n",
        "    ) -> Union[BasePromptDataset, ImagePromptDataset]:\n",
        "\n",
        "        if dataset_type == \"drawbench\":\n",
        "            return DrawBenchPromptDataset()\n",
        "        elif dataset_type == \"hps\":\n",
        "            return HPSV2PromptDataset()\n",
        "        elif dataset_type == \"imageandprompt\":\n",
        "            return ImagePromptDataset(**kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset type: '{dataset_type}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ehojpAJL8wV"
      },
      "source": [
        "6. Attack Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IxEwuOkML-FJ"
      },
      "outputs": [],
      "source": [
        "class BaseAttack(Attack):\n",
        "    \"\"\"\n",
        "    Small modifications to the torchattack's Attack class\n",
        "    to work with reward models\n",
        "    \"\"\"\n",
        "    def __init__(self, name, model):\n",
        "        \"\"\"\n",
        "        Initializes internal attack state.\n",
        "\n",
        "        Arguments:\n",
        "            name (str): name of attack.\n",
        "            model (BaseModel): model to attack.\n",
        "        \"\"\"\n",
        "\n",
        "        self.attack = name\n",
        "        self._attacks = OrderedDict()\n",
        "\n",
        "        self.set_model(model)\n",
        "        ################################################\n",
        "        # MODIFICATION\n",
        "        # Set device using torch.cuda instead of\n",
        "        # model.parameters().device\n",
        "        ################################################\n",
        "        try:\n",
        "              self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        ################################################\n",
        "        except Exception:\n",
        "            self.device = None\n",
        "            print(\"Failed to set device automatically, please try set_device() manual.\")\n",
        "\n",
        "        # Controls attack mode.\n",
        "        self.attack_mode = \"default\"\n",
        "        self.supported_mode = [\"default\"]\n",
        "        self.targeted = False\n",
        "        self._target_map_function = None\n",
        "\n",
        "        # Controls when normalization is used.\n",
        "        self.normalization_used = None\n",
        "        self._normalization_applied = None\n",
        "        if self.model.__class__.__name__ == \"RobModel\":\n",
        "            self._set_rmodel_normalization_used(model)\n",
        "\n",
        "        # Controls model mode during attack.\n",
        "        self._model_training = False\n",
        "        self._batchnorm_training = False\n",
        "        self._dropout_training = False\n",
        "\n",
        "    @wrapper_method\n",
        "    def _change_model_mode(self, given_training):\n",
        "        ################################################\n",
        "        # MODIFICATION\n",
        "        # do not iterate over model parameters\n",
        "        # as we use pipelines for inference\n",
        "        ################################################\n",
        "        pass\n",
        "        # if self._model_training:\n",
        "        #     self.model.train()\n",
        "        #     for _, m in self.model.named_modules():\n",
        "        #         if not self._batchnorm_training:\n",
        "        #             if \"BatchNorm\" in m.__class__.__name__:\n",
        "        #                 m = m.eval()\n",
        "        #         if not self._dropout_training:\n",
        "        #             if \"Dropout\" in m.__class__.__name__:\n",
        "        #                 m = m.eval()\n",
        "        # else:\n",
        "        #     self.model.eval()\n",
        "\n",
        "    @wrapper_method\n",
        "    def _recover_model_mode(self, given_training):\n",
        "        ################################################\n",
        "        # MODIFICATION\n",
        "        # do not execute model.train()\n",
        "        # as we use pipelines for inference\n",
        "        ################################################\n",
        "        if given_training:\n",
        "            pass\n",
        "            # self.model.train()\n",
        "\n",
        "    def __call__(self, inputs, labels=None, *args, **kwargs):\n",
        "      # given_training = self.model.training\n",
        "      # self._change_model_mode(given_training)\n",
        "\n",
        "      if self._normalization_applied is True:\n",
        "          inputs = self.inverse_normalize(inputs)\n",
        "          self._set_normalization_applied(False)\n",
        "\n",
        "          adv_inputs = self.forward(inputs, labels, *args, **kwargs)\n",
        "          # adv_inputs = self.to_type(adv_inputs, self.return_type)\n",
        "\n",
        "          adv_inputs = self.normalize(adv_inputs)\n",
        "          self._set_normalization_applied(True)\n",
        "      else:\n",
        "          adv_inputs = self.forward(inputs, labels, *args, **kwargs)\n",
        "          # adv_inputs = self.to_type(adv_inputs, self.return_type)\n",
        "\n",
        "      # self._recover_model_mode(given_training)\n",
        "\n",
        "      return adv_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "88t17iN8MpHI"
      },
      "outputs": [],
      "source": [
        "class GNRewardModel(BaseAttack):\n",
        "    \"\"\"\n",
        "    Gaussian Noise attack for reward models.\n",
        "\n",
        "    Arguments:\n",
        "        model (BaseModel): reward model to attack.\n",
        "        std (float): standard deviation of the Gaussian noise (Default: 0.1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, std=0.1):\n",
        "        super().__init__(\"GNReward\", model)\n",
        "        self.std = std\n",
        "        self.supported_mode = [\"default\"]\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        \"\"\"\n",
        "        Overridden forward method for attacking a reward model.\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = ImagePromptDataset(\n",
        "            image_list=images, prompt_list=labels,\n",
        "            image_transform_function=self.model.preprocess_function,\n",
        "            text_tokenizer_function=self.model.tokenizer\n",
        "        )\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=len(images), shuffle=False\n",
        "        )\n",
        "\n",
        "        images, _ = next(iter(dataloader))\n",
        "        images = images.clone().detach().to(self.device)\n",
        "        adv_images = images + self.std * torch.randn_like(images)\n",
        "        adv_images = torch.clamp(adv_images, min=0, max=1).detach()\n",
        "        return adv_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "76FmUnoUMsfM"
      },
      "outputs": [],
      "source": [
        "class FGSMRewardModel(BaseAttack):\n",
        "    \"\"\"\n",
        "    FGSM for reward models.\n",
        "\n",
        "    Instead of using cross-entropy, this attack uses a custom loss:\n",
        "    Loss = -reward, so that the adversary minimizes the reward score.\n",
        "\n",
        "    Distance Measure: Linf\n",
        "\n",
        "    Arguments:\n",
        "        model (BaseModel): reward model to attack.\n",
        "        eps (float): maximum perturbation. (Default: 8/255)\n",
        "        batch_size (int): batch size for processing images via DataLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, eps=8/255, batch_size=1):\n",
        "        super().__init__(\"FGSMRewardModel\", model)\n",
        "        self.eps = eps\n",
        "        self.batch_size = batch_size\n",
        "        self.supported_mode = [\"default\"]\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        \"\"\"\n",
        "        Overridden forward method for attacking a reward model using a DataLoader.\n",
        "        \"\"\"\n",
        "        dataset = ImagePromptDataset(\n",
        "            image_list=images, prompt_list=labels,\n",
        "            image_transform_function=self.model.preprocess_function,\n",
        "            text_tokenizer_function=None\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        adv_images_list = []\n",
        "\n",
        "        for images, labels in loader:\n",
        "            images = images.clone().detach().to(self.device)\n",
        "            images.requires_grad = True\n",
        "\n",
        "            reward = self.model.inference_with_grad(images, list(labels))\n",
        "            loss = -reward.mean()\n",
        "            grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n",
        "            adv_batch = images + self.eps * grad.sign()\n",
        "            adv_batch = torch.clamp(adv_batch, 0, 1).detach()\n",
        "            adv_images_list.append(adv_batch)\n",
        "\n",
        "        adv_images = torch.cat(adv_images_list, dim=0)\n",
        "        return adv_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pJblFYrlMvjL"
      },
      "outputs": [],
      "source": [
        "class PGDRewardModel(BaseAttack):\n",
        "    \"\"\"\n",
        "    PGD for reward models using global loss averaging over the entire dataset.\n",
        "\n",
        "    Instead of using cross-entropy loss, this attack uses a custom loss:\n",
        "    Loss = -reward, so that the adversary minimizes the reward score.\n",
        "\n",
        "    The entire dataset is loaded into memory, and during each PGD step we iterate\n",
        "    over mini-batches of the current adversarial images by simple slicing.\n",
        "\n",
        "    Distance Measure: Linf\n",
        "\n",
        "    Arguments:\n",
        "        model (nn.Module): reward model to attack.\n",
        "        eps (float): maximum perturbation (Default: 8/255).\n",
        "        alpha (float): step size (Default: 2/255).\n",
        "        steps (int): number of PGD steps (Default: 10).\n",
        "        random_start (bool): if True, initializes adversarial examples with a random perturbation.\n",
        "        batch_size (int): mini-batch size for computing the global loss (Default: 8).\n",
        "    \"\"\"\n",
        "    def __init__(self, model, eps=8/255, alpha=2/255, steps=10, random_start=True, batch_size=8):\n",
        "        super().__init__(\"PGDRewardModel\", model)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.steps = steps\n",
        "        self.random_start = random_start\n",
        "        self.batch_size = batch_size\n",
        "        self.supported_mode = [\"default\"]\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        \"\"\"\n",
        "        Overridden forward method for attacking a reward model.\n",
        "        \"\"\"\n",
        "        dataset = ImagePromptDataset(\n",
        "            image_list=images, prompt_list=labels,\n",
        "            image_transform_function=self.model.preprocess_function,\n",
        "            text_tokenizer_function=None\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        images_list = []\n",
        "        prompts_list = []\n",
        "\n",
        "        for imgs, prompts in loader:\n",
        "            images_list.append(imgs)\n",
        "            prompts_list.extend(prompts)\n",
        "\n",
        "        all_images = torch.cat(images_list, dim=0).to(self.device)\n",
        "        adv_images = all_images.clone().detach()\n",
        "        num_images = all_images.shape[0]\n",
        "\n",
        "        if self.random_start:\n",
        "            adv_images = adv_images + torch.empty_like(adv_images).uniform_(-self.eps, self.eps)\n",
        "            adv_images = torch.clamp(adv_images, 0, 1).detach()\n",
        "\n",
        "        for _ in range(self.steps):\n",
        "            global_grad = torch.zeros_like(adv_images)\n",
        "            total_samples = 0\n",
        "\n",
        "            for i in range(0, num_images, self.batch_size):\n",
        "                batch_images = adv_images[i: i + self.batch_size]\n",
        "                batch_prompts = prompts_list[i: i + self.batch_size]\n",
        "\n",
        "                batch_images.requires_grad_()\n",
        "\n",
        "                reward = self.model.inference_with_grad(batch_images, batch_prompts)\n",
        "                loss = -reward.sum() / num_images\n",
        "\n",
        "                grad = torch.autograd.grad(loss, batch_images)[0]\n",
        "                global_grad[i: i + self.batch_size] = grad\n",
        "\n",
        "                total_samples += batch_images.shape[0]\n",
        "\n",
        "                clear_cuda_memory_and_force_gc(force=True)\n",
        "\n",
        "            adv_images = adv_images.detach() + self.alpha * global_grad.sign()\n",
        "            delta = torch.clamp(adv_images - all_images, min=-self.eps, max=self.eps)\n",
        "            adv_images = torch.clamp(all_images + delta, 0, 1).detach()\n",
        "        return adv_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JZPJrAHtMy5u"
      },
      "outputs": [],
      "source": [
        "class SPSARewardModel(BaseAttack):\n",
        "    \"\"\"\n",
        "    SPSA attack for reward models.\n",
        "\n",
        "    Distance Measure : L_inf\n",
        "\n",
        "    Arguments:\n",
        "        model (nn.Module): Reward model to attack. It should take an image tensor and output a scalar reward.\n",
        "        eps (float): maximum perturbation. (Default: 0.3)\n",
        "        delta (float): smoothing parameter for gradient approximation. (Default: 0.01)\n",
        "        lr (float): learning rate for the optimizer. (Default: 0.01)\n",
        "        nb_iter (int): number of attack iterations. (Default: 1)\n",
        "        nb_sample (int): number of samples for SPSA gradient approximation. (Default: 128)\n",
        "        max_batch_size (int): maximum batch size for gradient estimation. (Default: 64)\n",
        "        batch_size (int): mini-batch size for processing inputs from the dataset. (Default: 8)\n",
        "    \"\"\"\n",
        "    def __init__(self, model, eps=0.3, delta=0.01, lr=0.01, nb_iter=1, nb_sample=128, max_batch_size=64, batch_size=8):\n",
        "        super().__init__(\"SPSARewardModel\", model)\n",
        "        self.eps = eps\n",
        "        self.delta = delta\n",
        "        self.lr = lr\n",
        "        self.nb_iter = nb_iter\n",
        "        self.nb_sample = nb_sample\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.dataset_batch_size = batch_size\n",
        "        self.supported_mode = [\"default\"]\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        dataset = ImagePromptDataset(\n",
        "            image_list=images, prompt_list=labels,\n",
        "            image_transform_function=self.model.preprocess_function,\n",
        "            text_tokenizer_function=self.model.tokenizer\n",
        "        )\n",
        "        dataloader = DataLoader(dataset, batch_size=self.dataset_batch_size, shuffle=False)\n",
        "\n",
        "        adv_images_list = []\n",
        "        for images, labels in dataloader:\n",
        "            images = images.clone().detach().to(self.device)\n",
        "            labels = labels.clone().detach().to(self.device)\n",
        "            adv = self.spsa_perturb(images, labels)\n",
        "            adv_images_list.append(adv)\n",
        "        return torch.cat(adv_images_list, dim=0)\n",
        "\n",
        "    def loss(self, images, labels):\n",
        "        reward = self.model.inference(images, labels)\n",
        "        reward = torch.tensor(reward, device=images.device)\n",
        "        return -reward.mean()\n",
        "\n",
        "    def linf_clamp_(self, dx, x, eps):\n",
        "        dx_clamped = torch.clamp(dx, min=-eps, max=eps)\n",
        "        x_adv = torch.clamp(x + dx_clamped, min=0, max=1)\n",
        "        # In-place update for proper optimizer tracking.\n",
        "        dx += x_adv - x - dx\n",
        "        return dx\n",
        "\n",
        "    def _get_batch_sizes(self, n, max_batch_size):\n",
        "        batches = [max_batch_size for _ in range(n // max_batch_size)]\n",
        "        if n % max_batch_size > 0:\n",
        "            batches.append(n % max_batch_size)\n",
        "        return batches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def spsa_grad(self, images, labels, delta, nb_sample, max_batch_size):\n",
        "        # images shape: (B, C, H, W)\n",
        "        grad = torch.zeros_like(images)\n",
        "        B = images.shape[0]\n",
        "\n",
        "        images = images.unsqueeze(1)   # (B, 1, C, H, W)\n",
        "        labels = labels.unsqueeze(1)   # (B, 1, P)\n",
        "\n",
        "        images = images.expand(B, max_batch_size, *images.shape[2:]).contiguous()  # (B, max_batch_size, C, H, W)\n",
        "        labels = labels.expand(B, max_batch_size, *labels.shape[2:]).contiguous()\n",
        "\n",
        "        v = torch.empty_like(images[:, :, :1, ...])  # (B, max_batch_size, 1, H, W)\n",
        "        for current_batch in self._get_batch_sizes(nb_sample, max_batch_size):\n",
        "            x_batch = images[:, :current_batch].contiguous()  # (B, current_batch, C, H, W)\n",
        "            y_batch = labels[:, :current_batch].contiguous()    # (B, current_batch, P)\n",
        "            v_batch = v[:, :current_batch]\n",
        "            v_batch.bernoulli_().mul_(2.0).sub_(1.0)\n",
        "            v_batch_exp = v_batch.expand_as(x_batch).contiguous()  # (B, current_batch, C, H, W)\n",
        "\n",
        "            B_curr, bs, C, H, W = x_batch.shape\n",
        "            x_batch_reshaped = x_batch.view(B_curr * bs, C, H, W)\n",
        "            y_batch_reshaped = y_batch.view(B_curr * bs, -1)\n",
        "            v_batch_reshaped = v_batch_exp.view(B_curr * bs, C, H, W)\n",
        "\n",
        "            df = self.loss(x_batch_reshaped + delta * v_batch_reshaped, y_batch_reshaped) \\\n",
        "                 - self.loss(x_batch_reshaped - delta * v_batch_reshaped, y_batch_reshaped)\n",
        "            df = df.view(-1, *([1] * (v_batch_reshaped.dim()-1)))\n",
        "            grad_batch = (df / (2.0 * delta)) * v_batch_reshaped # equivalent to original code as each element of v_batch_reshaped is +-1\n",
        "            grad_batch = grad_batch.view(B_curr, bs, C, H, W)\n",
        "            grad += grad_batch.sum(dim=1)\n",
        "\n",
        "        grad /= nb_sample\n",
        "        return grad\n",
        "\n",
        "    def spsa_perturb(self, x, y):\n",
        "        dx = torch.zeros_like(x)\n",
        "        dx.grad = torch.zeros_like(dx)\n",
        "        optimizer = torch.optim.Adam([dx], lr=self.lr)\n",
        "        for _ in range(self.nb_iter):\n",
        "            optimizer.zero_grad()\n",
        "            dx.grad = self.spsa_grad(x + dx, y, self.delta, self.nb_sample, self.max_batch_size)\n",
        "            optimizer.step()\n",
        "            dx = self.linf_clamp_(dx, x, self.eps)\n",
        "        x_adv = x + dx\n",
        "        return x_adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hioMTL4nQbct"
      },
      "outputs": [],
      "source": [
        "class AttackFactory:\n",
        "    @staticmethod\n",
        "    def create_attack(\n",
        "        attack_type: str,\n",
        "        model: BaseModel,\n",
        "        **kwargs,\n",
        "    ) -> BaseAttack:\n",
        "        if attack_type == \"gn\":\n",
        "            return GNRewardModel(model)\n",
        "        elif attack_type == \"fgsm\":\n",
        "            return FGSMRewardModel(model, **kwargs)\n",
        "        elif attack_type == \"pgd\":\n",
        "            return PGDRewardModel(model, **kwargs)\n",
        "        elif attack_type == \"spsa\":\n",
        "            return SPSARewardModel(model, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported attack type.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxzEzwrVfOyd"
      },
      "source": [
        "7. Define Arguments and Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2e5ywfpbgWdI"
      },
      "outputs": [],
      "source": [
        "def check_reward_model(value):\n",
        "    valid_versions = [\"v1.0\", \"v2.0\"]\n",
        "    if value not in valid_versions:\n",
        "        raise argparse.ArgumentTypeError(\n",
        "            \"reward_model_name must be one of: 'v1.0', 'v2.0'.\")\n",
        "    return value\n",
        "\n",
        "def check_dataset_name(value):\n",
        "    if value not in ['hps', 'drawbench']:\n",
        "        raise argparse.ArgumentTypeError(\n",
        "            \"dataset_name must be either 'hps' or 'drawbench'.\")\n",
        "    return value\n",
        "\n",
        "def check_attack_name(value):\n",
        "    if value not in ['gn', 'fgsm', 'pgd', 'spsa']:\n",
        "        raise argparse.ArgumentTypeError(\n",
        "            \"attack_name must be one of: 'gn', 'fgsm', 'pgd', 'spsa'.\")\n",
        "    return value\n",
        "\n",
        "def parse_attack_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Argument partser for attack process.\"\n",
        "    )\n",
        "\n",
        "    # Models group\n",
        "    models = parser.add_argument_group(\"models\")\n",
        "    models.add_argument(\"--reward_model_name\", type=check_reward_model, required=True,\n",
        "                        help=\"HPS reward model version: v1.0, v2.0\")\n",
        "    models.add_argument(\"--reward_threshold\", type=float, default=15.0,\n",
        "                        help=\"Minimum reward score for attack (default: 15.0)\")\n",
        "\n",
        "    # Datasets group\n",
        "    datasets = parser.add_argument_group(\"datasets\")\n",
        "    datasets.add_argument(\"--dataset_name\", type=check_dataset_name, required=True,\n",
        "                        help=\"Dataset for generating preliminary images: 'hps' or 'drawbench'\")\n",
        "    datasets.add_argument(\"--num_samples_per_category\", type=int, default=None,\n",
        "                        help=\"Number of text prompts per category (default: 5 for hps, 2 for drawbench)\")\n",
        "\n",
        "    # Attack group\n",
        "    attack = parser.add_argument_group(\"attack\")\n",
        "    attack.add_argument(\"--attack_name\", type=check_attack_name, required=True,\n",
        "                        help=\"Name of the perturbation attack: gn, fgsm, pgd, or spsa\")\n",
        "\n",
        "    # Misc group\n",
        "    misc = parser.add_argument_group(\"misc\")\n",
        "    misc.add_argument(\"--saved_images_path\", type=str, required=True,\n",
        "                        help=\"Path where base images and prompts are stored\")\n",
        "    misc.add_argument(\"--attack_batch_size\", type=int, default=8,\n",
        "                        help=\"Batch size for PGD & FGSM attack (default: 8)\")\n",
        "    misc.add_argument(\"--no_save_image_results\", dest=\"save_image_results\", action=\"store_false\",\n",
        "                        help=\"Do not store adversarial images, prompts, and reward scores\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    if args.num_samples_per_category is None:\n",
        "        if args.dataset_name == \"hps\":\n",
        "            args.num_samples_per_category = 5\n",
        "        else:  # drawbench\n",
        "            args.num_samples_per_category = 2\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3GkIDFImQNf7"
      },
      "outputs": [],
      "source": [
        "def compute_reward_statistics(top_k_prompts, adv_rewards):\n",
        "    \"\"\"\n",
        "    Compute reward statistics using both the original rewards from top_k_prompts\n",
        "    and the adversarial rewards in adv_rewards.\n",
        "\n",
        "    Parameters:\n",
        "        top_k_prompts (list): List of tuples (category, prompt, original_reward, image)\n",
        "        adv_rewards (list): List of adversarial rewards corresponding to each prompt.\n",
        "    \"\"\"\n",
        "    if not top_k_prompts or not adv_rewards:\n",
        "        return {\n",
        "            \"average_original\": 0.0,\n",
        "            \"average_adversarial\": 0.0,\n",
        "            \"per_category_original\": {},\n",
        "            \"per_category_adversarial\": {}\n",
        "        }\n",
        "\n",
        "    original_rewards = [entry[2] for entry in top_k_prompts]\n",
        "    avg_original = sum(original_rewards) / len(original_rewards)\n",
        "    avg_adv = sum(adv_rewards) / len(adv_rewards)\n",
        "\n",
        "    per_category_orig = {}\n",
        "    per_category_adv = {}\n",
        "\n",
        "    for (cat, _, orig_reward, _), adv_reward in zip(top_k_prompts, adv_rewards):\n",
        "        per_category_orig.setdefault(cat, []).append(orig_reward)\n",
        "        per_category_adv.setdefault(cat, []).append(adv_reward)\n",
        "\n",
        "    per_category_orig_avg = {cat: sum(scores) / len(scores) for cat, scores in per_category_orig.items()}\n",
        "    per_category_adv_avg = {cat: sum(scores) / len(scores) for cat, scores in per_category_adv.items()}\n",
        "\n",
        "    return {\n",
        "        \"average_original\": avg_original,\n",
        "        \"average_adversarial\": avg_adv,\n",
        "        \"per_category_original\": per_category_orig_avg,\n",
        "        \"per_category_adversarial\": per_category_adv_avg\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PAMY0Fwuhbt5"
      },
      "outputs": [],
      "source": [
        "def clear_cuda_memory_and_force_gc(force: bool = False):\n",
        "    \"\"\"\n",
        "    Clears the CUDA memory cache and forces garbage collection if the allocated memory\n",
        "    exceeds a certain threshold or if explicitly forced.\n",
        "\n",
        "    Args:\n",
        "        force (bool): If True, CUDA cache will be cleared and garbage collection\n",
        "                      will be forced regardless of the memory threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    memory_allocated = torch.cuda.max_memory_reserved()\n",
        "    memory_total = torch.cuda.get_device_properties(\"cuda\").total_memory\n",
        "\n",
        "    memory_threshold = memory_total * 0.7\n",
        "    if memory_allocated > memory_threshold or force:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nxaXvEhpxQMB"
      },
      "outputs": [],
      "source": [
        "def numerical_key(filename):\n",
        "    # Extract the number from filename like \"image_1.png\"\n",
        "    match = re.search(r'\\d+', filename)\n",
        "    return int(match.group()) if match else -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "guE0Pg4MhkIO"
      },
      "outputs": [],
      "source": [
        "class SampledDataset(Dataset):\n",
        "    def __init__(self, prompts, images=None, transforms=None):\n",
        "        self.data = [{\"category\": c, \"prompt\": p} for c, p in zip(prompts[\"category\"], prompts[\"prompt\"])]\n",
        "        self.images = images\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.images is not None:\n",
        "            return self.transforms(self.images[idx]), self.data[idx]\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B67ARZTDhpQg"
      },
      "source": [
        "8. Run Adversarial Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "q7yKvw83iT26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19867890-c6a5-449e-f4c2-e09c14773e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 890M/890M [00:21<00:00, 43.8MiB/s]\n",
            "<ipython-input-7-57db53084b7d>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(self.model_path)\n"
          ]
        }
      ],
      "source": [
        "reward_model = ModelFactory.create_model(\n",
        "    model_type=\"hpsv1\",\n",
        "    model_path=\"hpc.pt\" #HPS_v2_compressed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XBkKjVm-n-RQ"
      },
      "outputs": [],
      "source": [
        "to_pil = T.ToPILImage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UIQNlcqQhqdj"
      },
      "outputs": [],
      "source": [
        "def run_attack_rank_model(run_attack_args):\n",
        "    print(run_attack_args)\n",
        "\n",
        "    image_directory = run_attack_args.saved_images_path\n",
        "    prompts_file = os.path.join(image_directory, \"prompts.txt\")\n",
        "    prompts = {\"category\": [], \"prompt\": []}\n",
        "\n",
        "    with open(prompts_file, \"r\") as pf:\n",
        "        for line in pf:\n",
        "            content = line.split(\": \", 1)[1].strip()\n",
        "            if content.startswith(\"(\") and content.endswith(\")\"):\n",
        "                category, prompt = ast.literal_eval(content)\n",
        "                prompts[\"category\"].append(category)\n",
        "                prompts[\"prompt\"].append(prompt)\n",
        "\n",
        "    image_files = sorted([f for f in os.listdir(image_directory) if f.endswith(\".png\")], key=numerical_key)\n",
        "    original_images = [Image.open(os.path.join(image_directory, img_file)) for img_file in image_files]\n",
        "\n",
        "    dataset = SampledDataset(\n",
        "        prompts=prompts, images=original_images,\n",
        "        transforms=reward_model.preprocess_function\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "    all_results = []  # will store tuples: (category, prompt, reward_score)\n",
        "    total_batches = len(dataloader)\n",
        "    pbar = tqdm(total=total_batches, desc=\"Ranking images and prompts\")\n",
        "\n",
        "    for batch in dataloader:\n",
        "        images, prompts = batch\n",
        "        categories  = prompts[\"category\"]\n",
        "        prompt_texts = prompts[\"prompt\"]\n",
        "\n",
        "        reward_scores = reward_model.inference(inputs=images, captions=prompt_texts)\n",
        "        for cat, pr, score, image in zip(categories, prompt_texts, reward_scores, images):\n",
        "            all_results.append((cat, pr, score, to_pil(image.cpu())))\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "\n",
        "    filtered_results = [entry for entry in all_results if entry[2] >= run_attack_args.reward_threshold]\n",
        "    ranked_results = sorted(filtered_results, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    k = run_attack_args.num_samples_per_category * 4 if run_attack_args.dataset_name == \"hps\" else run_attack_args.num_samples_per_category * 11\n",
        "    top_k_prompts = ranked_results[:k]\n",
        "\n",
        "    print(\"Top prompts:\")\n",
        "    for idx, (cat, pr, score, _) in enumerate(top_k_prompts):\n",
        "        print(f\"Image {idx}: ({cat}, {pr}) with score {score}\")\n",
        "    return top_k_prompts, reward_model\n",
        "\n",
        "def run_attack_reward_model(run_attack_args, top_k_prompts, reward_model):\n",
        "    attack = AttackFactory.create_attack(\n",
        "        attack_type=run_attack_args.attack_name,\n",
        "        model=reward_model,\n",
        "        batch_size=run_attack_args.attack_batch_size\n",
        "    )\n",
        "\n",
        "    prompts = [(cat, pr) for cat, pr, _, _ in top_k_prompts]\n",
        "    prompts_only = [pr for _, pr, _, _ in top_k_prompts]\n",
        "    original_images = [image for _, _, _, image in top_k_prompts]\n",
        "    original_rewards = [score for _, _, score, _ in top_k_prompts]\n",
        "\n",
        "    adv_images = attack(\n",
        "        inputs=original_images,\n",
        "        labels=prompts,\n",
        "    )\n",
        "    adv_rewards = reward_model.inference(inputs=adv_images, captions=prompts_only)\n",
        "\n",
        "    if run_attack_args.save_image_results:\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "        parts = run_attack_args.saved_images_path.split(os.sep)\n",
        "        output_dir = f\"outputs/{parts[1]}/{run_attack_args.dataset_name}_adversarial/{run_attack_args.attack_name}/{run_attack_args.reward_model_name}/{timestamp}/\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        prompts_file_path = os.path.join(output_dir, \"prompts.txt\")\n",
        "        with open(prompts_file_path, \"w\") as pf:\n",
        "            for idx, (pr, orig_r, adv_r, adv_img) in enumerate(zip(prompts, original_rewards, adv_rewards, adv_images)):\n",
        "                image_filename = os.path.join(output_dir, f\"image_{idx}.png\")\n",
        "                pil_img = to_pil(adv_img.cpu())\n",
        "                pil_img.save(image_filename)\n",
        "                pf.write(f\"Image {idx}: ({repr(pr[0])}, {repr(pr[1])}, {orig_r}, {adv_r})\\n\")\n",
        "\n",
        "    stats = compute_reward_statistics(top_k_prompts, adv_rewards)\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"Overall Reward Statistics:\")\n",
        "    print(f\"  Original: {stats['average_original']} | Adversarial: {stats['average_adversarial']}\\n\")\n",
        "\n",
        "    print(\"Per-Category Comparison:\")\n",
        "    all_categories = set(stats[\"per_category_original\"].keys()).union(stats[\"per_category_adversarial\"].keys())\n",
        "    for cat in all_categories:\n",
        "        orig = stats[\"per_category_original\"].get(cat, 0)\n",
        "        adv = stats[\"per_category_adversarial\"].get(cat, 0)\n",
        "        print(f\"  {cat}: Original = {orig} | Adversarial = {adv}\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    clear_cuda_memory_and_force_gc(force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TVneiSHniC5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce632277-e2e8-41c7-866e-fb6c1979c6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(reward_model_name='v1.0', reward_threshold=15.0, dataset_name='drawbench', num_samples_per_category=2, attack_name='gn', saved_images_path='outputs/stable-diffusion-3-medium-diffusers/drawbench/2025-03-08-05-06-41', attack_batch_size=8, save_image_results=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ranking images and prompts: 100%|██████████| 6/6 [00:04<00:00,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top prompts:\n",
            "Image 0: (Reddit, Painting of the orange cat Otto von Garfield, Count of Bismarck-Schönhausen, Duke of Lauenburg, Minister-President of Prussia. Depicted wearing a Prussian Pickelhaube and eating his favorite meal - lasagna.) with score 24.515625\n",
            "Image 1: (Reddit, A baby fennec sneezing onto a strawberry, detailed, macro, studio light, droplets, backlit ears.) with score 23.40625\n",
            "Image 2: (Reddit, A church with stained glass windows depicting a hamburger and french fries.) with score 23.1875\n",
            "Image 3: (Gary Marcus et al. , An oil painting of a couple in formal evening wear going home get caught in a heavy downpour with no umbrellas.) with score 22.515625\n",
            "Image 4: (Reddit, A photo of a confused grizzly bear in calculus class.) with score 22.453125\n",
            "Image 5: (Conflicting, A horse riding an astronaut.) with score 22.21875\n",
            "Image 6: (Conflicting, A blue coloured pizza.) with score 22.015625\n",
            "Image 7: (DALL-E, A cube made of denim. A cube with the texture of denim.) with score 21.953125\n",
            "Image 8: (DALL-E, A triangular purple flower pot. A purple flower pot in the shape of a triangle.) with score 21.921875\n",
            "Image 9: (Positional, A wine glass on top of a dog.) with score 21.859375\n",
            "Image 10: (Text, A storefront with 'Text to Image' written on it.) with score 21.59375\n",
            "Image 11: (Positional, An umbrella on top of a spoon.) with score 21.578125\n",
            "Image 12: (DALL-E, A triangular orange picture frame. An orange picture frame in the shape of a triangle.) with score 21.484375\n",
            "Image 13: (Colors, A black colored dog.) with score 21.171875\n",
            "Image 14: (Conflicting, A pizza cooking an oven.) with score 20.984375\n",
            "Image 15: (Text, A storefront with 'Hello World' written on it.) with score 20.90625\n",
            "Image 16: (Colors, A black colored car.) with score 20.875\n",
            "Image 17: (Text, A storefront with 'NeurIPS' written on it.) with score 20.875\n",
            "Image 18: (Text, A storefront with 'Diffusion' written on it.) with score 20.796875\n",
            "Image 19: (DALL-E, A triangular pink stop sign. A pink stop sign in the shape of a triangle.) with score 20.609375\n",
            "Image 20: (Gary Marcus et al. , A grocery store refrigerator has pint cartons of milk on the top shelf, quart cartons on the middle shelf, and gallon plastic jugs on the bottom shelf.) with score 20.25\n",
            "Image 21: (Conflicting, A bird scaring a scarecrow.) with score 20.234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "sys.argv = [\n",
        "    \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "    \"--reward_model_name\", \"v1.0\",\n",
        "    \"--dataset_name\", \"drawbench\",\n",
        "    \"--attack_name\", \"gn\",\n",
        "    \"--saved_images_path\", \"outputs/stable-diffusion-3-medium-diffusers/drawbench/2025-03-08-05-06-41\",\n",
        "]\n",
        "\n",
        "args = parse_attack_args()\n",
        "top_k_prompts, reward_model = run_attack_rank_model(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q2k2oy0k_Dh"
      },
      "source": [
        "8.1 Gaussian Noise Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oz0jVGvZlBXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ae0593-4744-42d2-aeee-f27121ad92a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Overall Reward Statistics:\n",
            "  Original: 21.70028409090909 | Adversarial: 19.759943181818183\n",
            "\n",
            "Per-Category Comparison:\n",
            "  Colors: Original = 21.0234375 | Adversarial = 19.5859375\n",
            "  Positional: Original = 21.71875 | Adversarial = 18.984375\n",
            "  Reddit: Original = 23.390625 | Adversarial = 20.58984375\n",
            "  Conflicting: Original = 21.36328125 | Adversarial = 19.8046875\n",
            "  Text: Original = 21.04296875 | Adversarial = 19.70703125\n",
            "  DALL-E: Original = 21.4921875 | Adversarial = 19.55859375\n",
            "  Gary Marcus et al. : Original = 21.3828125 | Adversarial = 19.46875\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "sys.argv = [\n",
        "    \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "    \"--reward_model_name\", \"v1.0\",\n",
        "    \"--dataset_name\", \"drawbench\",\n",
        "    \"--attack_name\", \"gn\",\n",
        "    \"--saved_images_path\", \"outputs/stable-diffusion-3-medium-diffusers/drawbench/2025-03-08-05-06-41\",\n",
        "]\n",
        "\n",
        "args = parse_attack_args()\n",
        "run_attack_reward_model(args, top_k_prompts, reward_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMynoi29opEd"
      },
      "source": [
        "8.2 FGSM Attack (Aggregate reward score over batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ip9bUWeMooko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5496f4-5eab-4fae-e5d1-a4ed6d9fcea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Overall Reward Statistics:\n",
            "  Original: 21.70028409090909 | Adversarial: 19.7578125\n",
            "\n",
            "Per-Category Comparison:\n",
            "  Colors: Original = 21.0234375 | Adversarial = 19.5234375\n",
            "  Positional: Original = 21.71875 | Adversarial = 18.890625\n",
            "  Reddit: Original = 23.390625 | Adversarial = 20.60546875\n",
            "  Conflicting: Original = 21.36328125 | Adversarial = 19.74609375\n",
            "  Text: Original = 21.04296875 | Adversarial = 19.81640625\n",
            "  DALL-E: Original = 21.4921875 | Adversarial = 19.50390625\n",
            "  Gary Marcus et al. : Original = 21.3828125 | Adversarial = 19.578125\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "sys.argv = [\n",
        "    \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "    \"--reward_model_name\", \"v1.0\",\n",
        "    \"--dataset_name\", \"drawbench\",\n",
        "    \"--attack_name\", \"fgsm\",\n",
        "    \"--attack_batch_size\", \"8\",\n",
        "    \"--saved_images_path\", \"outputs/stable-diffusion-3-medium-diffusers/drawbench/2025-03-08-05-06-41\",\n",
        "]\n",
        "\n",
        "args = parse_attack_args()\n",
        "run_attack_reward_model(args, top_k_prompts, reward_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci3MBO4_rKwF"
      },
      "source": [
        "8.3 FGSM Attack (Update each image individually)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XEVbBE3DrVyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e68a12-76de-4631-82ad-b5616925a5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Overall Reward Statistics:\n",
            "  Original: 21.70028409090909 | Adversarial: 19.7578125\n",
            "\n",
            "Per-Category Comparison:\n",
            "  Colors: Original = 21.0234375 | Adversarial = 19.5234375\n",
            "  Positional: Original = 21.71875 | Adversarial = 18.890625\n",
            "  Reddit: Original = 23.390625 | Adversarial = 20.60546875\n",
            "  Conflicting: Original = 21.36328125 | Adversarial = 19.74609375\n",
            "  Text: Original = 21.04296875 | Adversarial = 19.81640625\n",
            "  DALL-E: Original = 21.4921875 | Adversarial = 19.50390625\n",
            "  Gary Marcus et al. : Original = 21.3828125 | Adversarial = 19.578125\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "sys.argv = [\n",
        "    \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "    \"--reward_model_name\", \"v1.0\",\n",
        "    \"--dataset_name\", \"drawbench\",\n",
        "    \"--attack_name\", \"fgsm\",\n",
        "    \"--attack_batch_size\", \"1\",\n",
        "    \"--saved_images_path\", \"outputs/stable-diffusion-3-medium-diffusers/drawbench/2025-03-08-05-06-41\",\n",
        "]\n",
        "\n",
        "args = parse_attack_args()\n",
        "run_attack_reward_model(args, top_k_prompts, reward_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n9DjNidrdIO"
      },
      "source": [
        "8.4 PGD Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "gtt9QY7CvVSK"
      },
      "outputs": [],
      "source": [
        "clear_cuda_memory_and_force_gc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ezp13K3brfTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d4e5b2-9694-40c1-d1a8-5fb755c99821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Overall Reward Statistics:\n",
            "  Original: 21.70028409090909 | Adversarial: 16.986150568181817\n",
            "\n",
            "Per-Category Comparison:\n",
            "  Colors: Original = 21.0234375 | Adversarial = 17.9375\n",
            "  Positional: Original = 21.71875 | Adversarial = 15.66796875\n",
            "  Reddit: Original = 23.390625 | Adversarial = 16.74609375\n",
            "  Conflicting: Original = 21.36328125 | Adversarial = 16.478515625\n",
            "  Text: Original = 21.04296875 | Adversarial = 17.8046875\n",
            "  DALL-E: Original = 21.4921875 | Adversarial = 17.130859375\n",
            "  Gary Marcus et al. : Original = 21.3828125 | Adversarial = 16.921875\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "sys.argv = [\n",
        "    \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "    \"--reward_model_name\", \"v1.0\",\n",
        "    \"--dataset_name\", \"drawbench\",\n",
        "    \"--attack_name\", \"pgd\",\n",
        "    \"--attack_batch_size\", \"2\",\n",
        "    \"--saved_images_path\", \"outputs/stable-diffusion-3-medium-diffusers/drawbench/2025-03-08-05-06-41\",\n",
        "]\n",
        "\n",
        "args = parse_attack_args()\n",
        "run_attack_reward_model(args, top_k_prompts, reward_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYM6Kw9jry-j"
      },
      "source": [
        "8.5 SPSA Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tFkqtL_Ur1iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d6316d-36a2-4fa3-bee1-10a75a7a806c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Overall Reward Statistics:\n",
            "  Original: 21.70028409090909 | Adversarial: 19.76846590909091\n",
            "\n",
            "Per-Category Comparison:\n",
            "  Colors: Original = 21.0234375 | Adversarial = 19.546875\n",
            "  Positional: Original = 21.71875 | Adversarial = 18.90625\n",
            "  Reddit: Original = 23.390625 | Adversarial = 20.58203125\n",
            "  Conflicting: Original = 21.36328125 | Adversarial = 19.765625\n",
            "  Text: Original = 21.04296875 | Adversarial = 19.82421875\n",
            "  DALL-E: Original = 21.4921875 | Adversarial = 19.5546875\n",
            "  Gary Marcus et al. : Original = 21.3828125 | Adversarial = 19.546875\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "sys.argv = [\n",
        "    \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "    \"--reward_model_name\", \"v1.0\",\n",
        "    \"--dataset_name\", \"drawbench\",\n",
        "    \"--attack_name\", \"spsa\",\n",
        "    \"--saved_images_path\", \"outputs/stable-diffusion-3-medium-diffusers/drawbench/2025-03-08-05-06-41\",\n",
        "]\n",
        "\n",
        "args = parse_attack_args()\n",
        "run_attack_reward_model(args, top_k_prompts, reward_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46c5e568aa7643f8b16d6b1b0d0efd3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9c099061f8f445380058f199c5ef60d",
              "IPY_MODEL_c05b5360df3a4c76a31a82c34172aa3f",
              "IPY_MODEL_49599243926646419c2884e4924b3ccf"
            ],
            "layout": "IPY_MODEL_b19e7adf6e7b459898f81e452be8af19"
          }
        },
        "f9c099061f8f445380058f199c5ef60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a94b3b693047d9aad25af39bffcead",
            "placeholder": "​",
            "style": "IPY_MODEL_48b87674d3f247c398924a6e08310da3",
            "value": ""
          }
        },
        "c05b5360df3a4c76a31a82c34172aa3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d154a3e810c7489fa118b4dde220e765",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f446e65889c04727873ca716257afc61",
            "value": 0
          }
        },
        "49599243926646419c2884e4924b3ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_350e831c3e9344fcaed2583c3b73f029",
            "placeholder": "​",
            "style": "IPY_MODEL_09bccfe2c1424bc4b59276418e99f099",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "b19e7adf6e7b459898f81e452be8af19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1a94b3b693047d9aad25af39bffcead": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b87674d3f247c398924a6e08310da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d154a3e810c7489fa118b4dde220e765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f446e65889c04727873ca716257afc61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "350e831c3e9344fcaed2583c3b73f029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09bccfe2c1424bc4b59276418e99f099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}