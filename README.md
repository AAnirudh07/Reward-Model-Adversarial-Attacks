# Reward-Model-Adversarial-Attacks

### Motivation
The motivation behind this preliminary analysis is to evaluate the robustness of reward models that score text-to-image model outputs based on human preference. It examines if small adversarial perturbations can cause a high-scoring image to drop in score and whether such perturbations transfer between different reward models. It also explores which diffusion models are more prone to these attacks and investigates the underlying factors contributing to their vulnerability.



### References
1. Wu, X., Sun, K., Zhu, F., Zhao, R., & Li, H. (2023). Human Preference Score: Better Aligning Text-to-Image Models with Human Preference. ArXiv. https://arxiv.org/abs/2303.14420